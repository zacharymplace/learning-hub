{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Z$ Learning Hub","text":"<p>Welcome\u2014this is my working notebook for patterns I reuse: data wrangling, finance ops, docs, and creative tooling.</p> <p>Goal: ship copy-paste-ready building blocks with brief rationale and sane defaults.</p>"},{"location":"#start-here","title":"Start here","text":"<ul> <li>How-To: short, practical guides you can follow in 5\u201310 minutes.</li> <li>Snippets: reusable code blocks with minimal dependencies.</li> <li>Reference: naming, structure, and conventions that keep things tidy.</li> </ul>"},{"location":"#whats-inside","title":"What\u2019s inside","text":"<ul> <li>Pandas I/O Cheats \u2013 fast patterns for CSV/Excel/Parquet, types, and perf.</li> <li>Mermaid \u2192 PNG \u2013 export diagrams for decks or SOPs without local tooling.</li> <li>Snippets (SQL/Python) \u2013 ready to drop into day-to-day work.</li> </ul> <p>Have an idea? Open an issue with the label <code>proposal</code>.</p>"},{"location":"#examples","title":"Examples","text":"<p>Download CSV \u00b7 Download Parquet</p> <p>See the full guide: Power Query Hand-off Patterns</p>"},{"location":"404/","title":"Page not found","text":"<p>This page doesn\u2019t exist (anymore).</p> <ul> <li>Go Home </li> <li>See How-To guides </li> <li>Grab an example dataset</li> </ul> <p>Tip: use the search in the header.</p>"},{"location":"examples/","title":"Example datasets","text":"<p>Download the sample files used in the Power Query hand-off guide:</p> <p>Download CSV Download Parquet</p> <p>Tip: Right-click \u2192 \u201cSave link as\u2026\u201d if your browser tries to open them inline.</p>"},{"location":"decisions/","title":"Architecture Decision Records (ADRs)","text":"<ul> <li>Create a new ADR with timestamp, title, status, context, decision, consequences.</li> <li>File format: <code>YYYYMMDD-meaningful-title.md</code></li> </ul>"},{"location":"examples/examples/","title":"Example datasets","text":"<p>Download the sample files used in the Power Query hand-off guide:</p> <p>Download CSV Download Parquet</p> <p>Tip: Right-click \u2192 \u201cSave link as\u2026\u201d if your browser opens them inline.</p>"},{"location":"how-to/forecast-rollups/","title":"Forecast rollups (monthly \u2192 quarterly)","text":"<p>Aggregate monthly actuals/forecast into QTD/YTD and fiscal quarters.</p>"},{"location":"how-to/forecast-rollups/#inputs","title":"Inputs","text":"<ul> <li><code>entity</code>, <code>dept</code> (text)</li> <li><code>month</code> (date-like; EOM recommended)</li> <li><code>amount</code> (numeric)</li> </ul>"},{"location":"how-to/forecast-rollups/#pandas-recipe-calendar-fiscal","title":"Pandas recipe (calendar &amp; fiscal)","text":"<pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/examples/forecast_monthly.csv\", parse_dates=[\"month\"])\n\n# Calendar quarter totals\ndf[\"quarter\"] = df[\"month\"].dt.to_period(\"Q\")\nq = df.groupby([\"entity\",\"dept\",\"quarter\"], as_index=False)[\"amount\"].sum()\n\n# QTD (within-quarter running total)\ndf[\"qtd\"] = df.groupby(\n    [\"entity\",\"dept\", df[\"month\"].dt.to_period(\"Q\")]\n)[\"amount\"].cumsum()\n\n# YTD\ndf[\"year\"] = df[\"month\"].dt.year\ndf[\"ytd\"] = df.groupby([\"entity\",\"dept\",\"year\"])[\"amount\"].cumsum()\n\n# Fiscal quarters starting July (Q-JUN)\ndf[\"fquarter\"] = df[\"month\"].dt.to_period(\"Q-JUN\")\nfq = df.groupby([\"entity\",\"dept\",\"fquarter\"], as_index=False)[\"amount\"].sum()\nprint(q.head(), fq.head())\n</code></pre>"},{"location":"how-to/forecast-rollups/#sql-sketch-postgres","title":"SQL sketch (Postgres)","text":"<pre><code>SELECT\n  entity,\n  dept,\n  DATE_TRUNC('quarter', month)::date AS quarter_start,\n  SUM(amount) AS amount_q\nFROM fact_monthly\nGROUP BY 1,2,3;\n-- Fiscal quarters: shift by 6 months then DATE_TRUNC('quarter'), shift back.\n</code></pre>"},{"location":"how-to/forecast-rollups/#checks","title":"Checks","text":"<ul> <li>Totals tie to monthly source</li> <li>Partial quarters handled (missing months = 0)</li> <li>Rounding rules documented</li> </ul> <p>Examples: see <code>forecast_monthly.csv</code> in Examples</p>"},{"location":"how-to/forecast-rollups/#sql-recipes","title":"SQL recipes","text":""},{"location":"how-to/forecast-rollups/#postgres","title":"Postgres","text":"<pre><code>-- Calendar quarter totals\nSELECT\n  entity,\n  dept,\n  date_trunc('quarter', month)::date AS quarter_start,\n  SUM(amount) AS amount_q\nFROM fact_monthly\nGROUP BY 1,2,3;\n\n-- QTD / YTD running totals\nSELECT\n  entity, dept, month::date,\n  SUM(amount) OVER (\n    PARTITION BY entity, dept, date_trunc('quarter', month)\n    ORDER BY month\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS amount_qtd,\n  SUM(amount) OVER (\n    PARTITION BY entity, dept, date_trunc('year', month)\n    ORDER BY month\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS amount_ytd\nFROM fact_monthly;\n\n-- Fiscal quarters starting July (shift 6 months)\nSELECT\n  entity, dept,\n  (date_trunc('quarter', month - INTERVAL '6 months') + INTERVAL '6 months')::date AS fiscal_q_start,\n  SUM(amount) AS amount_fq\nFROM fact_monthly\nGROUP BY 1,2,3;\n</code></pre>"},{"location":"how-to/forecast-rollups/#sql-server","title":"SQL Server","text":"<pre><code>-- Calendar quarter totals\nSELECT\n  entity,\n  dept,\n  DATEADD(QUARTER, DATEDIFF(QUARTER, 0, [month]), 0) AS quarter_start,\n  SUM(amount) AS amount_q\nFROM fact_monthly\nGROUP BY entity, dept, DATEADD(QUARTER, DATEDIFF(QUARTER, 0, [month]), 0);\n\n-- QTD / YTD running totals\nSELECT\n  entity, dept, [month],\n  SUM(amount) OVER (\n    PARTITION BY entity, dept, YEAR([month]), DATEPART(QUARTER, [month])\n    ORDER BY [month]\n    ROWS UNBOUNDED PRECEDING\n  ) AS amount_qtd,\n  SUM(amount) OVER (\n    PARTITION BY entity, dept, YEAR([month])\n    ORDER BY [month]\n    ROWS UNBOUNDED PRECEDING\n  ) AS amount_ytd\nFROM fact_monthly;\n\n-- Fiscal quarters starting July (shift 6 months)\nSELECT\n  entity, dept,\n  DATEADD(MONTH, 6,\n    DATEADD(QUARTER, DATEDIFF(QUARTER, 0, DATEADD(MONTH, -6, [month])), 0)\n  ) AS fiscal_q_start,\n  SUM(amount) AS amount_fq\nFROM fact_monthly\nGROUP BY entity, dept,\n  DATEADD(MONTH, 6,\n    DATEADD(QUARTER, DATEDIFF(QUARTER, 0, DATEADD(MONTH, -6, [month])), 0)\n  );\n</code></pre>"},{"location":"how-to/lease-csv-cleanup/","title":"Lease CSV cleanup (ASC 842 prep)","text":"<p>Normalize vendor lease exports for consistent PQ/Power BI import.</p>"},{"location":"how-to/lease-csv-cleanup/#steps","title":"Steps","text":"<ol> <li>Normalize headers to snake_case.</li> <li>Parse dates (commencement/end/payment) and strip timezone.</li> <li>Coerce types (numbers, booleans).</li> <li>Derive fields (e.g., annualized rent).</li> <li>Export CSV or Parquet with stable types.</li> </ol>"},{"location":"how-to/lease-csv-cleanup/#python-recipe","title":"Python recipe","text":"<pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/examples/leases_sample.csv\", dtype_backend=\"pyarrow\", encoding=\"utf-8\")\ndf.columns = (df.columns.str.strip().str.lower().str.replace(r\"[^a-z0-9]+\",\"_\", regex=True))\n\nfor c in [\"commencement_date\",\"end_date\"]:\n    if c in df:\n        df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None)\n\nbool_cols = [\"auto_renew\",\"is_active\"]\nfor c in bool_cols:\n    if c in df:\n        df[c] = df[c].astype(\"boolean\")\n\ndf[\"monthly_rent\"] = pd.to_numeric(df.get(\"monthly_rent\"), errors=\"coerce\")\ndf[\"annualized_rent\"] = (df[\"monthly_rent\"] * 12).round(2)\n\ndf.to_parquet(\"docs/examples/leases_clean.parquet\", index=False)\ndf.to_csv(\"docs/examples/leases_clean.csv\", index=False, encoding=\"utf-8\", lineterminator=\"\\n\")\n</code></pre>"},{"location":"how-to/lease-csv-cleanup/#power-query-notes","title":"Power Query notes","text":"<ul> <li>For CSV: Transform \u2192 Data Type \u2192 Using Locale\u2026, Decimal (en-US), Date for date cols.</li> <li>Keep IDs with leading zeros as Text.</li> </ul> <p>Examples: <code>leases_sample.csv</code> in Examples. df.to_parquet(\"docs/examples/leases_clean.parquet\", index=False) df.to_csv(\"docs/examples/leases_clean.csv\", index=False, encoding=\"utf-8\", lineterminator=\"\\n\")</p>"},{"location":"how-to/mermaid-export/","title":"Mermaid \u2192 PNG (Browser-Only)","text":"<p>Use the Mermaid Live Editor to export as PNG/SVG\u2014no local installs.</p> <ol> <li>Go to <code>https://mermaid.live/</code></li> <li>Paste your diagram.</li> <li><code>Export</code> \u2192 <code>PNG</code> (or <code>SVG</code> for slides).</li> </ol>"},{"location":"how-to/mermaid-export/#example","title":"Example","text":"<pre><code>flowchart TD\n  A[Import] --&gt; B[Transform]\n  B --&gt; C[Validate]\n  C --&gt; D[Load]</code></pre>"},{"location":"how-to/mermaid-gallery/","title":"Mermaid Gallery","text":"<p>Quick, reusable diagrams for finance ops.</p>"},{"location":"how-to/mermaid-gallery/#etl-pipeline","title":"ETL pipeline","text":"<pre><code>flowchart LR\n  A[\"Source data\"] --&gt; B[\"Import\"]\n  B --&gt; C[\"Transform\"]\n  C --&gt; D[\"Validate\"]\n  D --&gt; E[\"Load to target\"]\n  C --&gt; C1[\"Type casts&lt;br/&gt;normalize headers\"]\n  C --&gt; C2[\"Deduplicate&lt;br/&gt;join dims\"]\n  D --&gt; D1[\"Row counts&lt;br/&gt;null checks\"]\n  D --&gt; D2[\"Schema check\"]</code></pre>"},{"location":"how-to/mermaid-gallery/#close-hub-overview","title":"Close Hub Overview","text":"<pre><code>flowchart TD\n  subgraph S[\"Close Hub\"]\n    R[\"Bank recs\"] --&gt; TB[\"Trial balance\"]\n    AP[\"AP close\"] --&gt; TB\n    AR[\"AR close\"] --&gt; TB\n    JE[\"JEs prepared/reviewed\"] --&gt; TB\n    TB --&gt; FL[\"Flux/variance&lt;br/&gt;review\"]\n    FL --&gt; PK[\"Package &amp; sign-off\"]\n  end</code></pre>"},{"location":"how-to/mermaid-gallery/#ramp-quarterly-access-review-qar","title":"Ramp Quarterly Access Review (QAR)","text":"<pre><code>flowchart LR\n  X[\"Export users &amp; roles\"] --&gt; Y[\"Prep review file\"]\n  Y --&gt; M[\"Manager review\"]\n  M --&gt; E[\"Exceptions queue\"]\n  M --&gt; OK[\"No exceptions\"]\n  E --&gt; C[\"Controller/CISO review\"]\n  OK --&gt; Cert[\"Controller certifies\"]\n  C --&gt; Cert\n  Cert --&gt; Arc[\"Archive + audit log\"]</code></pre>"},{"location":"how-to/pandas-io/","title":"Pandas I/O Cheats","text":"<p>TL;DR</p> <p>Prefer Parquet for speed &amp; types. For CSVs, always set dtypes and date parsing.</p>"},{"location":"how-to/pandas-io/#read","title":"Read","text":"<pre><code>import pandas as pd\n\ndf_csv = pd.read_csv(\n    \"in.csv\",\n    dtype={\"account_id\": \"string\", \"amount\": \"float64\"},\n    parse_dates=[\"txn_date\", \"posted_date\"],\n    dayfirst=False,  # US style\n    encoding=\"utf-8\",\n)\n\ndf_xlsx = pd.read_excel(\"in.xlsx\", sheet_name=0, dtype_backend=\"pyarrow\")\ndf_parq = pd.read_parquet(\"in.parquet\")\n</code></pre>"},{"location":"how-to/pandas-io/#write","title":"Write","text":"<pre><code>df_csv.to_csv(\"out.csv\", index=False)\ndf_parq.to_parquet(\"out.parquet\", index=False)\n</code></pre>"},{"location":"how-to/pandas-io/#common-fixes","title":"Common Fixes","text":"<pre><code># normalize headers\ndf.columns = (\n    df.columns\n      .str.strip()\n      .str.lower()\n      .str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n)\n\n# coerce numbers w/ visibility\ndf[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n\n# consistent date zone-naive\nfor c in [\"txn_date\", \"posted_date\"]:\n    df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None)\n</code></pre>"},{"location":"how-to/pandas-io/#csv-essentials","title":"CSV essentials","text":"<pre><code>import csv, pandas as pd\n\n# Read with stable types\ndf = pd.read_csv(\n    \"in.csv\",\n    dtype_backend=\"pyarrow\",                # arrow-backed dtypes\n    na_values=[\"\", \"NA\", \"N/A\", \"null\"],    # keep default NA handling + custom\n    keep_default_na=True,\n    encoding=\"utf-8\",\n    parse_dates=[\"txn_date\", \"posted_date\"],# if present\n    # thousands=\",\"                         # uncomment if your numbers use commas\n    # usecols=[\"id\",\"amount\",\"txn_date\"]    # load only needed columns\n)\n\n# Write with sane defaults\ndf.to_csv(\n    \"out.csv\",\n    index=False,\n    encoding=\"utf-8\",\n    quoting=csv.QUOTE_MINIMAL,\n    lineterminator=\"\\n\"\n)\n</code></pre>"},{"location":"how-to/pandas-io/#large-files-streaming","title":"Large files (streaming)","text":"<pre><code>import pandas as pd\n\nreader = pd.read_csv(\n    \"big.csv\",\n    dtype_backend=\"pyarrow\",\n    usecols=[\"id\",\"amount\",\"ts\"],\n    parse_dates=[\"ts\"],\n    chunksize=200_000\n)\n\nfor i, chunk in enumerate(reader):\n    # process/clean per chunk here if needed\n    chunk.to_parquet(f\"parquet/part_{i:03}.parquet\", index=False)  # snappy by default\n</code></pre>"},{"location":"how-to/pandas-io/#parquet-essentials","title":"Parquet essentials","text":"<pre><code># Write\ndf.to_parquet(\"out.parquet\", index=False, compression=\"snappy\")  # or \"gzip\"\n\n# Read\ndf2 = pd.read_parquet(\"out.parquet\", dtype_backend=\"pyarrow\")\n</code></pre>"},{"location":"how-to/pandas-io/#simple-partitioning-idea","title":"Simple partitioning idea","text":"<p>If you already have a quarter or year column, write separate files per value (as in the chunks example) to simulate a small \u201cdataset.\u201d</p>"},{"location":"how-to/pandas-io/#performance-types-tips","title":"Performance &amp; types tips","text":"<pre><code># Categorical for low-cardinality text (saves memory)\ndf[\"dept\"] = df[\"dept\"].astype(\"category\")\n\n# Ensure true booleans\nfor c in [\"is_active\",\"is_recurring\"]:\n    if c in df: df[c] = df[c].astype(\"boolean\")\n</code></pre> <ul> <li>Keep IDs with leading zeros as string (not int).</li> <li>Dates: store/export ISO <code>YYYY-MM-DD</code>; strip timezones for Excel/PQ hand-offs.</li> <li>Prefer Parquet for back-and-forth with Python/Power BI to preserve types.</li> </ul>"},{"location":"how-to/pandas-io/#remote-io-quick-notes","title":"Remote I/O (quick notes)","text":"<pre><code># HTTP/HTTPS\npd.read_csv(\"https://example.com/data.csv\")\n\n# S3 (requires s3fs and fsspec installed)\npd.read_parquet(\"s3://my-bucket/path/file.parquet\")\n</code></pre>"},{"location":"how-to/pandas-io/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li>Thousand separators in CSV (set <code>thousands=\",\"</code> on read).</li> <li>Mixed booleans like <code>\"TRUE\"</code>, <code>\"False\"</code>, <code>\"0/1\"</code> \u2192 cast to <code>boolean</code>.</li> <li>Locale surprises in PQ: use Using Locale\u2026 when importing CSV, or use Parquet.</li> </ul>"},{"location":"how-to/power-query-handoff/","title":"Power Query Hand-off Patterns","text":"<p>Clean, predictable transfers from Python/SQL into Excel Power Query (PQ).</p> <p>TL;DR</p> <ul> <li>Prefer Parquet when possible: better types, fewer surprises.  </li> <li>If using CSV, control the schema (dtypes, date formats, locale) and document it.</li> </ul>"},{"location":"how-to/power-query-handoff/#goals","title":"Goals","text":"<ul> <li>Preserve data types (dates, numbers, booleans, text) across the Python \u2192 Excel PQ boundary.</li> <li>Avoid silent type drift (e.g., 0123 \u2192 123, True/False \u2192 text).</li> <li>Provide copy-paste recipes for both CSV and Parquet workflows.</li> </ul>"},{"location":"how-to/power-query-handoff/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+ with pandas (and pyarrow for Parquet).</li> <li>Excel with Power Query (Get &amp; Transform).  </li> <li>A consistent standard schema for your exports.</li> </ul>"},{"location":"how-to/power-query-handoff/#recommended-workflow","title":"Recommended workflow","text":"<pre><code>flowchart LR\n  A[\"Source (DB/CSV/XLSX)\"] --&gt; B[\"Python Transform\"]\n  B --&gt; C{\"Export Format\"}\n  C --&gt;|Parquet| D[\".parquet&lt;br/&gt;types preserved\"]\n  C --&gt;|CSV| E[\".csv&lt;br/&gt;+ schema + locale\"]\n  D --&gt; F[\"Power Query&lt;br/&gt;From Parquet\"]\n  E --&gt; G[\"Power Query&lt;br/&gt;From Text/CSV&lt;br/&gt;Using Locale + Types\"]\n  F --&gt; H[\"Model/Report\"]\n  G --&gt; H</code></pre>"},{"location":"how-to/power-query-handoff/#standard-schema-example","title":"Standard schema (example)","text":"<p>Use a dict to lock types. Adjust to your domain.</p> <pre><code>SCHEMA = {\n    \"txn_id\": \"string\",\n    \"account_id\": \"string\",\n    \"merchant\": \"string\",\n    \"memo\": \"string\",\n    \"category\": \"string\",\n    \"currency\": \"string\",\n    \"amount\": \"float64\",\n}\n\nDATE_COLS = [\"txn_date\", \"posted_date\"]\nBOOL_COLS = [\"is_recurring\", \"is_refund\"]\nORDER = [\"txn_id\", \"txn_date\", \"posted_date\", \"account_id\", \"merchant\",\n         \"memo\", \"category\", \"currency\", \"amount\", \"is_recurring\", \"is_refund\"]\n</code></pre>"},{"location":"how-to/power-query-handoff/#python-parquet-preferred","title":"Python \u2192 Parquet (preferred)","text":"<pre><code>import pandas as pd\n\ndef export_parquet(df: pd.DataFrame, path: str) -&gt; None:\n    # Normalize headers\n    df.columns = (df.columns\n                  .str.strip()\n                  .str.lower()\n                  .str.replace(r\"[^a-z0-9]+\", \"_\", regex=True))\n\n    # Enforce schema\n    for col, typ in SCHEMA.items():\n        if col not in df.columns:\n            df[col] = pd.Series([None] * len(df), dtype=\"string\" if typ == \"string\" else typ)\n        else:\n            df[col] = df[col].astype(typ)\n\n    # Dates: parse &amp; drop timezone so Excel shows the expected day\n    for c in DATE_COLS:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None)\n\n    # Booleans\n    for c in BOOL_COLS:\n        if c in df.columns:\n            df[c] = df[c].astype(\"boolean\")\n\n    # Column order (optional)\n    present = [c for c in ORDER if c in df.columns]\n    df = df[present + [c for c in df.columns if c not in present]]\n\n    # Write Parquet (pyarrow engine)\n    df.to_parquet(path, index=False)\n</code></pre> <p>Power Query steps (Excel): 1. Data \u2192 Get Data \u2192 From File \u2192 From Parquet. 2. Browse to your .parquet export and Load.    Power Query will infer logical/number/date correctly from Parquet metadata.</p>"},{"location":"how-to/power-query-handoff/#python-csv-controlled","title":"Python \u2192 CSV (controlled)","text":"<pre><code>import pandas as pd\n\ndef export_csv(df: pd.DataFrame, path: str) -&gt; None:\n    # Same normalization &amp; typing as Parquet path above...\n    # (call the same functions or reuse code)\n\n    # Date formatting for CSV: ISO date only (prevents midnight shifts)\n    for c in [col for col in DATE_COLS if col in df.columns]:\n        df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n\n    df.to_csv(\n        path,\n        index=False,\n        encoding=\"utf-8\",\n        lineterminator=\"\\n\",\n    )\n</code></pre> <p>Power Query steps (Excel) for CSV: 1. Data \u2192 Get Data \u2192 From Text/CSV \u2192 pick your file. 2. In the preview dialog, choose File Origin/Delimiter if needed. Click Transform Data. 3. In PQ:    - Home \u2192 Use First Row as Headers (if needed).    - Transform \u2192 Detect Data Type (avoid); instead set types explicitly:      - Transform \u2192 Data Type \u2192 Using Locale\u2026        - For amount: Decimal Number, Locale: English (United States).        - For dates: Date.        - For booleans: set to True/False (logical) or convert 0/1 \u2192 logical via Replace Values then type to True/False.    - If IDs like 00123 must keep leading zeros, set those columns to Text (don\u2019t convert to number).</p>"},{"location":"how-to/power-query-handoff/#power-query-m-examples","title":"Power Query M examples","text":"<p>CSV with locale + types</p> <pre><code>let\n  Source = Csv.Document(\n    File.Contents(\"C:\\path\\export.csv\"),\n    [Delimiter=\",\", Columns=11, Encoding=65001, QuoteStyle=QuoteStyle.Csv]\n  ),\n  Promote = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),\n  // Set types with explicit locale to control decimal/thousand separators\n  Types = Table.TransformColumnTypes(\n    Promote,\n    {\n      {\"txn_id\", type text},\n      {\"txn_date\", type date},\n      {\"posted_date\", type date},\n      {\"account_id\", type text},\n      {\"merchant\", type text},\n      {\"memo\", type text},\n      {\"category\", type text},\n      {\"currency\", type text},\n      {\"amount\", type number},\n      {\"is_recurring\", type logical},\n      {\"is_refund\", type logical}\n    },\n    \"en-US\"\n  )\nin\n  Types\n</code></pre> <p>Parquet</p> <pre><code>let\n  Source = Parquet.Document(File.Contents(\"C:\\path\\export.parquet\")),\n  // Usually already typed; add explicit casts if you want hard guarantees:\n  Typed = Table.TransformColumnTypes(\n    Source,\n    {\n      {\"txn_date\", type date},\n      {\"posted_date\", type date},\n      {\"amount\", type number}\n    }\n  )\nin\n  Typed\n</code></pre>"},{"location":"how-to/power-query-handoff/#pitfalls-fixes","title":"Pitfalls &amp; fixes","text":"<ul> <li>Datetime shifts (TZ) Symptom: dates appear one day off in Excel. Fix (Python): <pre><code>df[c] = pd.to_datetime(df[c], errors=\"coerce\").dt.tz_localize(None)\n</code></pre></li> <li>Leading zeros lost Symptom: 00123 becomes 123. Fix: keep IDs as text in Python (dtype=\"string\") and set Text in PQ.</li> <li>Decimal or thousand separators misread Symptom: 1,234.56 becomes 123456 or text. Fix: In PQ, use Using Locale \u2192 Decimal Number with en-US.</li> <li>Booleans become text Symptom: True/False shows as \"True\"/\"False\". Fix: In PQ, set type to True/False. From Python, use pandas boolean dtype.</li> <li>CSV encoding issues Symptom: special characters garbled. Fix: write encoding=\"utf-8\"; in PQ, encoding 65001 (UTF-8).</li> </ul>"},{"location":"how-to/power-query-handoff/#verification-checklist-1-min","title":"Verification checklist (1 min)","text":"<ul> <li>Row count matches between Python and PQ.  </li> <li>Key columns have expected types in PQ (Date, Decimal Number, True/False, Text).  </li> <li>Spot-check a few rows for:</li> <li>Leading zeros on IDs</li> <li>Amounts with decimals</li> <li>Dates matching originals</li> <li>Booleans as checkboxes</li> </ul>"},{"location":"how-to/power-query-handoff/#deliverables-when-you-close-this-issue","title":"Deliverables (when you close this issue)","text":"<ul> <li>This page: docs/how-to/power-query-handoff.md</li> <li>(Optional) Sample files in /examples/:</li> <li>examples/pq_handoff_sample.parquet</li> <li>examples/pq_handoff_sample.csv</li> <li>Link this page from the How-To nav in mkdocs.yml:   <pre><code>nav:\n  - Home: index.md\n  - How-To:\n      - Power Query Hand-off Patterns: how-to/power-query-handoff.md\n      - Pandas I/O Cheats: how-to/pandas-io.md\n</code></pre></li> </ul>"},{"location":"how-to/power-query-handoff/#faq","title":"FAQ","text":"<p>Why not always use Parquet? Parquet is best for fidelity and size, but some teams still need CSV for hand edits or legacy tools. This guide keeps CSV safe when required.</p> <p>Do I need utf-8-sig? Usually no. Use utf-8. If another system demands BOM, write encoding=\"utf-8-sig\" and set PQ encoding to 65001.</p> <p>Can Power Query read Parquet on SharePoint/OneDrive? Yes\u2014use the Parquet connector with the file path/URL your tenant supports, or land the file locally/SharePoint and select it via Get Data \u2192 From Parquet.</p> <p>Examples: see the downloadable datasets in Examples.</p> <p>Last updated: {{ date | strftime('%Y-%m-%d') }}</p>"},{"location":"reference/conventions/","title":"Conventions","text":""},{"location":"reference/conventions/#repo-layout","title":"Repo layout","text":"<ul> <li><code>docs/</code> \u2013 published site content</li> <li><code>docs/how-to/</code>, <code>docs/snippets/</code>, <code>docs/reference/</code>, <code>docs/examples/</code></li> <li><code>scripts/python/</code> \u2013 generators &amp; utilities</li> <li><code>.github/</code> \u2013 workflows, templates</li> </ul>"},{"location":"reference/conventions/#filenames-headings","title":"Filenames &amp; headings","text":"<ul> <li>Kebab-case for docs: <code>power-query-handoff.md</code></li> <li>One H1 per page; short H2/H3s; verbs for How-Tos (\u201cExport\u2026\u201d, \u201cValidate\u2026\u201d)</li> </ul>"},{"location":"reference/conventions/#code-style","title":"Code style","text":"<ul> <li>Python: ruff/black defaults, pure functions, JSONL logs, <code>pyarrow</code> dtypes</li> <li>SQL: UPPER keywords, snake_case identifiers, idempotent staging/merge</li> <li>Markdown: copy-paste blocks first; minimal prose; use admonitions for tips/pitfalls</li> </ul>"},{"location":"reference/conventions/#data-types-cross-tool","title":"Data types (cross-tool)","text":"<ul> <li>Dates: store/export ISO <code>YYYY-MM-DD</code>, strip TZ for Excel hand-offs</li> <li>IDs: keep as text if leading zeros matter</li> <li>Booleans: pandas <code>boolean</code>; Excel/PQ logical</li> </ul>"},{"location":"reference/conventions/#commits","title":"Commits","text":"<ul> <li><code>docs(how-to): \u2026</code>, <code>docs(snippets): \u2026</code>, <code>chore(examples): \u2026</code></li> <li>Use <code>closes #123</code> to auto-close issues</li> </ul>"},{"location":"reference/conventions/#links-paths","title":"Links &amp; paths","text":"<ul> <li>From top-level pages \u2192 <code>examples/&lt;file&gt;</code></li> <li>From <code>how-to/*</code> pages \u2192 <code>../examples/&lt;file&gt;</code></li> </ul>"},{"location":"reference/conventions/#quality-checks","title":"Quality checks","text":"<ul> <li>MkDocs build with <code>--strict</code> must be green</li> <li>Add examples for anything users copy/paste</li> </ul>"},{"location":"snippets/python/","title":"Python","text":""},{"location":"snippets/python/#robust-csv-loader-schema-aware","title":"Robust CSV loader (schema-aware)","text":"<pre><code>from __future__ import annotations\nimport pandas as pd\n\ndef read_csv_schema(path: str, schema: dict[str, str], date_cols: list[str] = None) -&gt; pd.DataFrame:\n    date_cols = date_cols or []\n    df = pd.read_csv(path, dtype=schema, parse_dates=date_cols, encoding=\"utf-8\")\n    # Normalize headers\n    df.columns = (\n        df.columns.str.strip().str.lower().str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n    )\n    # Enforce columns exist\n    for col, typ in schema.items():\n        if col not in df.columns:\n            df[col] = pd.Series([None] * len(df), dtype=\"string\" if typ == \"string\" else typ)\n    return df\n</code></pre>"},{"location":"snippets/python/#minimal-audit-log-writer","title":"Minimal Audit Log Writer","text":"<p><pre><code>from datetime import datetime\nfrom pathlib import Path\nimport json\n\ndef write_audit_event(outdir: str, event: dict) -&gt; None:\n    Path(outdir).mkdir(parents=True, exist_ok=True)\n    event[\"ts\"] = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n</code></pre>     with open(Path(outdir) / \"audit_log.jsonl\", \"a\", encoding=\"utf-8\") as f:         f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")</p>"},{"location":"snippets/python/#read_csv-with-yaml-schema","title":"read_csv with YAML schema","text":"<p>Use a YAML file to enforce dtypes and parsed dates at read time.</p> <pre><code># docs/examples/schema.yaml\ndtypes:\n  txn_id: string\n  account_id: string\n  amount: float64\ndates: [txn_date, posted_date]\n</code></pre> <pre><code>import yaml, pandas as pd\n\nwith open(\"docs/examples/schema.yaml\") as f:\n    s = yaml.safe_load(f)\n\ndf = pd.read_csv(\n    \"docs/examples/in.csv\",\n    dtype=s[\"dtypes\"],\n    parse_dates=s.get(\"dates\", []),\n    encoding=\"utf-8\"\n)\ndf.columns = (df.columns.str.strip().str.lower()\n              .str.replace(r\"[^a-z0-9]+\",\"_\", regex=True))\n</code></pre>"},{"location":"snippets/sql/","title":"SQL","text":""},{"location":"snippets/sql/#safe-date-window","title":"Safe date window","text":"<pre><code>WHERE txn_date &gt;= DATEADD(day, -30, CAST(GETDATE() AS date))\n</code></pre>"},{"location":"snippets/sql/#null-safe-join-key","title":"Null-safe join key","text":"<pre><code>ON COALESCE(a.key, '') = COALESCE(b.key, '')\n</code></pre>"},{"location":"snippets/sql/#idempotent-staging","title":"Idempotent staging","text":"<pre><code>TRUNCATE TABLE stg_import;\nBULK INSERT stg_import FROM 'blob/path.csv' WITH (FIRSTROW=2, FIELDTERMINATOR=',', ROWTERMINATOR='\\n');\nMERGE dim_merchant AS d\nUSING stg_import AS s\nON d.merchant_id = s.merchant_id\nWHEN MATCHED THEN UPDATE SET d.name = s.name\nWHEN NOT MATCHED THEN INSERT (merchant_id, name) VALUES (s.merchant_id, s.name);\n</code></pre>"}]}